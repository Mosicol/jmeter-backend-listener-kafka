input {
  kafka {
    bootstrap_servers => "localhost:9092"
    topics => [ "test" ]
    codec => "json"
    group_id => "GRP_TEST_LOGSTASH_P_1"
    auto_offset_reset => "earliest"
  }
}

filter {
  json {
    source => "message"
  }

  # Logstash has strange implementation for json filter above.
  # If it sees a field named @timestamp, then it expects it to be a seconds based value (UNIX rather than UNIX_MS) and tries to parse on its own to reuse as event's @timestamp.
  # In our case, @timestamp is milli-seconds based long, hence it fails to parse and causes a warning with _timestampparsefailure tag.

  if "_timestampparsefailure" in [tags] {
    date {
      match => [ "_@timestamp", "UNIX_MS" ]
      remove_tag => "_timestampparsefailure"
      remove_field => "_@timestamp"
    }
  }

  ruby {
    code => "if event.get('tags').length == 0 then event.remove('tags') end"
  }
}

output {
  #stdout { codec => rubydebug }

  elasticsearch {
    index => "jmeter_metrics-%{+xxxx-ww}"  # Weekly index
    hosts => ["localhost:9201"]
    user => "elastic"
    password => "elastic"
  }
}
